// readme

The step size is set to be 2.0

Please refer to the runtime recording below:
1081047490 Byte = 1.0068 GB = 1030.96 MB

Rumtime: twice becuase of two streams?
batch_size		runtime	(s)		only IO?		Throughput of IO	Comment
2048			16.772825 		No, 			61.466 MB/s			with kernel calculation
2048			15.545481		Yes. 			66.319 MB/s			only use readLSAReview to a buffer 

The percentage of parsing and IO v.s. total with calculation is
15.545481/16.772825 = 92.68%
final error rate is around 10%

batch_size		latency	(s)				total runtime (s)	throughput
1 				out of time				(>100s)				<0.01
32				10.710475586			27.821920			2.99
1024			1.663786621				17.699587			615.50
2048			0.827701477				16.672071			2474.33

When the batch_size got big, somewhere in the for loop there is an illegal memory
access from the second stream.

Because the number of iterations get reduced, the final result and accuracy of weight suffer.

I changed the grid_size to be fixed at 512 (should be sufficient), and the large batch now
can be fit into the gpu. I don;t know if this is the way you want us to debug, but it somehow
worked. I feel that the original calculation of grid_size is reasonable, but when batch_size get large, 
it will stuck in the second stream at random iteration numbers. (batch_size = 8192 is the limit)

batch_size		latency	(s)				total runtime (s)	throughput
16384
65536
