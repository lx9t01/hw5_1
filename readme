// readme

The step size is set to be 2.0

Please refer to the runtime recording below:
1081047490 Byte = 1.0068 GB = 1030.96 MB

Rumtime: twice becuase of two streams?
batch_size		runtime	(s)		only IO?		Throughput of IO	Comment
2048			16.772825 		No, 			61.466 MB/s			with kernel calculation
2048			15.545481		Yes. 			66.319 MB/s			only use readLSAReview to a buffer 

The percentage of parsing and IO v.s. total with calculation is
15.545481/16.772825 = 92.68%
final error rate is around 10%

batch_size		latency	(s)				total runtime (s)	throughput
1 				out of time				(>100s)				<0.01
32				10.710475586			27.821920			2.99
1024			1.663786621				17.699587			615.50
2048			0.827701477				16.672071			2474.33

When the batch_size got big, somewhere in the for loop there is an illegal memory
access from the second stream.

Because the number of iterations get reduced, the final result and accuracy of weight suffer.

cuda-gdb gave an 
Error: Failed to suspend device (dev=2, error=1).
and it was discussed that it could because of staying in the kernel for too long (too complecated loop)
(when I run the kernel on cuda-GDB, the latency is super huge, when using gdb, there is NO illegal memory
access assert!!! This is super weird.)

I guess the reason is that when batch get big, the time it takes for kernel to run get longer, and somehow
the program is not waiting for the kernel to finish running.
There is suggestions like add a 
cudaDeviceSynchronize()
after every kernel call. 

batch_size		latency	(s)				total runtime (s)	throughput
4096			0.429809998				16.547529			9530.01
16384
65536
